{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0652f264-b2c3-43b2-9310-7c00e50e7cdc",
   "metadata": {},
   "source": [
    "# XGBoost.Dask in many threads\n",
    "\n",
    "Sometimes we want to train many large XGBoost models in parallel.  We do so in this example with ...\n",
    "\n",
    "1.  The `xgboost.dask` project to do large training runs\n",
    "2.  Optuna to do hyper-parameter-optimization\n",
    "3.  A thread pool, to run many of these in parallel\n",
    "4.  Coiled to launch Dask clusters (but you could swap in your favorite Dask deployment technology as you like)\n",
    "\n",
    "Using `xgboost.dask` from many threads tooks a couple of small tweaks across projects.  This notebook resulted in the following PRs and issues:\n",
    "\n",
    "-  https://github.com/dask/distributed/issues/7377\n",
    "-  https://github.com/dask/dask/pull/9723\n",
    "-  https://github.com/dask/distributed/pull/7369\n",
    "-  https://github.com/dmlc/xgboost/pull/8558 (mostly cosmetic, not necessary)\n",
    "-  Also something in Coiled to allow package_sync to be thread-safe, should be released by 2022-12-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179b003-e075-4be5-bde1-4ce90b7d726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH=\"s3://prefect-dask-examples/nyc-uber-lyft/feature_table_fixed_upper_bound.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b929987-5089-4c13-be35-c1812d65fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from coiled import Cluster\n",
    "import coiled\n",
    "\n",
    "import optuna\n",
    "from dask_ml.metrics import mean_squared_error as lazy_mse\n",
    "import xgboost as xgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "\n",
    "from dask_ml.datasets import make_classification_df\n",
    "from dask_ml.model_selection import train_test_split, KFold\n",
    "from dask_ml.preprocessing import OneHotEncoder\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from s3fs import S3FileSystem\n",
    "from xgboost.core import XGBoostError\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780815b1-fa82-43c1-93c0-572f0f6182c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, coiled\n",
    "print(\"coiled:\", coiled.__version__)\n",
    "print(\"dask:\", dask.__version__)\n",
    "print(\"dask.distributed:\", dask.distributed.__version__)\n",
    "print(\"optuna:\", optuna.__version__)\n",
    "print(\"xgboost:\", xgb.__version__)\n",
    "print(\"coiled:\", coiled.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd461c3-e551-420a-a3e1-1ca2515bb613",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf70461-6ab3-429b-a0a1-29f6b162894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we subset data for cross-validation\n",
    "\n",
    "def _make_cv(df, num_folds):\n",
    "    frac = [1 / num_folds]*num_folds\n",
    "    splits = df.random_split(frac, shuffle=True)\n",
    "    for i in range(num_folds):\n",
    "        train = [splits[j] for j in range(num_folds) if j != i]\n",
    "        test = splits[i]\n",
    "        yield train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132568f4-ac1b-43ff-85fd-e5a5324a38a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(trial_number, study_params, n_splits=5, cluster_name = None, teardown_cluster=False):\n",
    "    if cluster_name is None:\n",
    "        thread_id = threading.get_ident()\n",
    "        cluster_name = \"xgb-nyc-taxi-\" + str(thread_id)\n",
    "\n",
    "    cluster = coiled.Cluster(\n",
    "        worker_vm_types=[\"m6i.4xlarge\"],\n",
    "        scheduler_vm_types=[\"m6i.2xlarge\"],\n",
    "        package_sync=True, # copy local packages,\n",
    "        name=cluster_name,\n",
    "        shutdown_on_close=False,  # reuse cluster across runs\n",
    "        show_widget=False,\n",
    "        n_workers=10,\n",
    "        use_best_zone=True,\n",
    "        account=\"dask-engineering\",\n",
    "        backend_options={\"region\": \"us-east-2\", \"spot\": True, \"spot_on_demand_fallback\": True}\n",
    "        )\n",
    "\n",
    "    print(\"starting run\")\n",
    "    with Client(cluster) as client:\n",
    "        print(client.id)\n",
    "        with client.as_current():\n",
    "            \n",
    "            # Load and pre-process the DataFrame\n",
    "            ddf = dd.read_parquet(FILEPATH)\n",
    "            ddf = ddf.repartition(partition_size=\"128MB\").persist()\n",
    "            categorical_vars = ddf.select_dtypes(include=\"category\")\n",
    "            ddf = ddf.categorize(columns=categorical_vars)\n",
    "            \n",
    "\n",
    "            val_scores = []\n",
    "\n",
    "            \n",
    "            for i, (train, test) in enumerate(_make_cv(ddf, n_splits)):\n",
    "                print(f\"Starting training run {i}\")\n",
    "                start = datetime.datetime.now()\n",
    "                train = dd.concat(train)\n",
    "                try:\n",
    "                    assert all(train[c].cat.known for c in categorical_vars)\n",
    "                    assert all(test[c].cat.known for c in categorical_vars)\n",
    "                except Exception as e:\n",
    "                    cluster.shutdown()\n",
    "                    raise RuntimeError(f\"Categorical_vars are not known\")\n",
    "\n",
    "                y_train = train['trip_time'].to_frame()\n",
    "                y_train = y_train.astype(float).persist()\n",
    "                X_train = train.drop(columns=['trip_time']).persist()\n",
    "\n",
    "                # Make the training data\n",
    "                y_test = test['trip_time'].to_frame().persist()\n",
    "                y_test = y_test.astype(float).persist()\n",
    "                X_test = test.drop(columns='trip_time').persist()\n",
    "\n",
    "                try:\n",
    "                    print(\"Make dtrain\")\n",
    "                    dtrain = DaskDMatrix(client, X_train, y_train, enable_categorical=True)\n",
    "\n",
    "                    print(\"Make dtest\")\n",
    "                    dtest = DaskDMatrix(client, X_test, y_test, enable_categorical=True)\n",
    "\n",
    "                    print(\"Training model\")\n",
    "\n",
    "                    model = xgb.dask.train(\n",
    "                        client,\n",
    "                        {\n",
    "                            'verbosity': 2,\n",
    "                            'tree_method': 'hist', \n",
    "                            \"objective\": \"reg:squarederror\",\n",
    "                            **study_params\n",
    "                        },\n",
    "                        dtrain,\n",
    "                        num_boost_round=4,\n",
    "                        evals=[(dtrain, \"train\")],\n",
    "                    )\n",
    "\n",
    "                    print(\"Make predictions\")\n",
    "                    # It's faster to run the prediction directly on X_test DataFrame\n",
    "                    # We also need to confirm that predictions on dtest when it\n",
    "                    # contains categoricals performs as expected\n",
    "                    predictions = xgb.dask.predict(client, model, X_test)\n",
    "\n",
    "                    print(\"Score the model\")\n",
    "                    score = lazy_mse(y_test.astype(\"float32\").to_dask_array(lengths=True).reshape(-1,), \n",
    "                                     predictions.to_dask_array(lengths=True), squared=False,\n",
    "                                    )\n",
    "                    print(f\"rmse_score:  {score}\")\n",
    "                    val_scores.append(score)\n",
    "                    print(f\"val_scores:  {val_scores}\")\n",
    "\n",
    "                except XGBoostError as e:\n",
    "                    print(f\"Trial {i} failed with {e}\")\n",
    "        return np.mean(val_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb43aa7-6744-4490-8b17-467ef2565239",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_options = dict(\n",
    "    n_splits = 5 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250d23a-4d6c-4758-9200-f47a0a614bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 75, 125),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.6),\n",
    "        'subsample': trial.suggest_float('subsample', 0, 1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0, 1),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0, 1),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 0.5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "    }\n",
    "    rmse = train_model(\n",
    "        trial_number=trial.number,\n",
    "        study_params=params, \n",
    "        n_splits=train_options[\"n_splits\"],\n",
    "    )\n",
    "    print(f\"final mse:  {rmse}\")\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3b355-7e76-423f-8b4c-8e35eb1eb022",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a single study\n",
    "\n",
    "study = optuna.create_study(study_name=\"nyc-taxi-study\")\n",
    "\n",
    "executor = ThreadPoolExecutor(4)\n",
    "\n",
    "futures = [\n",
    "    executor.submit(study.optimize, objective, n_trials=3) for _ in range(20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52306d15-ccdf-4314-900a-70fb02e8a2e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5a2bf-c88c-4266-9738-73cb6fcb9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6da34-582c-4e2d-9b58-89a5369d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4928342-4749-4e29-bb65-ccabd8190a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f913de4-9988-49d9-9b23-f5a90ecb9973",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb4134-5218-4deb-a2bf-f52d3ca6c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c41f7-afa1-44a7-ab36-d1ed15a4bd48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(study, \"data/second_study.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7bdd4-e1aa-4118-bc5f-d0d591ea4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = futures[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991bb84-15e5-4102-aa18-7f19b94509ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0007a-cfbf-4a10-999b-ecb4067ca901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
