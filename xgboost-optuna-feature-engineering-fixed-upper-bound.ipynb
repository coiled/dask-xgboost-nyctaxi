{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0652f264-b2c3-43b2-9310-7c00e50e7cdc",
   "metadata": {},
   "source": [
    "# XGBoost.Dask in many threads\n",
    "\n",
    "Sometimes we want to train many large XGBoost models in parallel.  We do so in this example with ...\n",
    "\n",
    "1.  The `xgboost.dask` project to do large training runs\n",
    "2.  Optuna to do hyper-parameter-optimization\n",
    "3.  A thread pool, to run many of these in parallel\n",
    "4.  Coiled to launch Dask clusters (but you could swap in your favorite Dask deployment technology as you like)\n",
    "\n",
    "Using `xgboost.dask` from many threads tooks a couple of small tweaks across projects.  This notebook resulted in the following PRs and issues:\n",
    "\n",
    "-  https://github.com/dask/distributed/issues/7377\n",
    "-  https://github.com/dask/dask/pull/9723\n",
    "-  https://github.com/dask/distributed/pull/7369\n",
    "-  https://github.com/dmlc/xgboost/pull/8558 (mostly cosmetic, not necessary)\n",
    "-  Also something in Coiled to allow package_sync to be thread-safe, should be released by 2022-12-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b929987-5089-4c13-be35-c1812d65fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from coiled import Cluster\n",
    "import coiled\n",
    "\n",
    "import optuna\n",
    "from dask_ml.metrics import mean_squared_error as lazy_mse\n",
    "import xgboost as xgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "\n",
    "from dask_ml.datasets import make_classification_df\n",
    "from dask_ml.model_selection import train_test_split, KFold\n",
    "from dask_ml.preprocessing import OneHotEncoder\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780815b1-fa82-43c1-93c0-572f0f6182c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, coiled\n",
    "print(\"coiled:\", coiled.__version__)\n",
    "print(\"dask:\", dask.__version__)\n",
    "print(\"dask.distributed:\", dask.distributed.__version__)\n",
    "print(\"optuna:\", optuna.__version__)\n",
    "print(\"xgboost:\", xgb.__version__)\n",
    "print(\"coiled:\", coiled.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a504d-4d53-448b-b12b-1c37f5769598",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 = 1415.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd461c3-e551-420a-a3e1-1ca2515bb613",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd3bf0-f8c2-4240-b12b-8ee6e4c772c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOROUGH_MAPPING = {\n",
    "    \"Manhattan\": \"Superborough 1\",\n",
    "    \"Bronx\": \"Superborough 1\",\n",
    "    \"EWR\": \"Superborough 1\",\n",
    "    \"Brooklyn\": \"Superborough 2\",\n",
    "    \"Queens\": \"Superborough 2\",\n",
    "    \"Staten Island\": \"Superborough 3\",\n",
    "    \"Unknown\": \"Unknown\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8574767-4b4c-4c82-a31d-448bd2d6ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"loading data\")\n",
    "    to_exclude=[\"string\", \"category\", \"object\"]\n",
    "    ddf= dd.read_parquet(\"s3://prefect-dask-examples/nyc-uber-lyft/processed_files.parquet\")\n",
    "    ddf = ddf.assign(accessible_vehicle = 1)\n",
    "    print(\"Make accessible feature\")\n",
    "    ddf.accessible_vehicle = ddf.accessible_vehicle.where(ddf.on_scene_datetime.isnull(),0)  # Only applies if the vehicle is wheelchair accessible\n",
    "    ddf = ddf.assign(pickup_month = ddf.pickup_datetime.dt.month)\n",
    "    ddf = ddf.assign(pickup_dow = ddf.pickup_datetime.dt.dayofweek)\n",
    "    ddf = ddf.assign(pickup_hour = ddf.pickup_datetime.dt.hour)\n",
    "    \n",
    "    ddf = ddf.drop(columns=['on_scene_datetime', 'request_datetime',\n",
    "                            'pickup_datetime', 'dispatching_base_num',\n",
    "                            'originating_base_num', 'shared_request_flag',\n",
    "                           'shared_match_flag','dropoff_datetime',\n",
    "                            'base_passenger_fare', 'bcf', 'sales_tax',\n",
    "                            'tips', 'driver_pay', 'access_a_ride_flag',\n",
    "                            'wav_match_flag',\n",
    "                           ]\n",
    "                  )\n",
    "\n",
    "    ddf = ddf.dropna(how=\"any\")\n",
    "    ddf = ddf.repartition(partition_size=\"128MB\").persist()\n",
    "    ddf = ddf.reset_index(drop=True)\n",
    "\n",
    "    original_rowcount = len(ddf.index)\n",
    "\n",
    "    # Remove outliers\n",
    "    # Based on our earlier EDA, we will set the lower bound at zero, which is consistent with our\n",
    "    # domain knowledge that no trip should have a duration less than zero.  We calculate the upper_bound\n",
    "    # and filter the IQR\n",
    "    lower_bound = 0\n",
    "    upper_bound = Q3 + (1.5*(Q3 - lower_bound))\n",
    "    \n",
    "    ddf = ddf.loc[(ddf['trip_time'] >= lower_bound) & (ddf['trip_time'] <= upper_bound)]\n",
    "    \n",
    "    ddf = ddf.repartition(partition_size=\"128MB\").persist()\n",
    "    print(f\"Fraction of dataset left after removing outliers:  {len(ddf.index) / original_rowcount}\")\n",
    "\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af41ab-17d9-47fd-81b8-7fda50bf8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_superborough(df):\n",
    "    PUSuperborough = [BOROUGH_MAPPING.get(i) for i in df.PUBorough.tolist()]\n",
    "    DOSuperborough = [BOROUGH_MAPPING.get(i) for i in df.DOBorough.tolist()]\n",
    "    cross_superborough = [\"N\" if i==j else \"Y\" for (i,j) in zip(PUSuperborough, DOSuperborough)]\n",
    "    return df.assign(CrossSuperborough = cross_superborough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51335c28-de29-4a23-ba2e-e7d389acff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_taxi_data(ddf):\n",
    "    print(\"Load taxi data\")\n",
    "    taxi_df = pd.read_csv(\"data/taxi+_zone_lookup.csv\", usecols=[\"LocationID\", \"Borough\"])\n",
    "\n",
    "    ddf = dd.merge(ddf, taxi_df, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"inner\")\n",
    "    ddf = ddf.rename(columns={\"Borough\": \"PUBorough\"})\n",
    "    ddf = ddf.drop(columns=\"LocationID\")\n",
    "\n",
    "    ddf = dd.merge(ddf, taxi_df, left_on=\"DOLocationID\", right_on=\"LocationID\", how=\"inner\")\n",
    "    ddf = ddf.rename(columns={\"Borough\": \"DOBorough\"})\n",
    "    ddf = ddf.drop(columns=\"LocationID\")  \n",
    "    \n",
    "    print(\"Make superboroughs\")\n",
    "    ddf = ddf.map_partitions(lambda df: get_superborough(df))\n",
    "    ddf['airport_fee'] = ddf['airport_fee'].replace(\"None\", 0)\n",
    "    ddf['airport_fee'] = ddf['airport_fee'].replace('nan', 0)\n",
    "    ddf['airport_fee'] = ddf['airport_fee'].astype(float)\n",
    "    ddf['airport_fee'] = ddf['airport_fee'].fillna(0)\n",
    "\n",
    "    ddf = ddf.repartition(partition_size=\"128MB\").persist()\n",
    "\n",
    "    print(\"Make categoricals\")\n",
    "    categories = ['hvfhs_license_num', 'PULocationID', \"DOLocationID\", 'wav_request_flag',\n",
    "                  'accessible_vehicle', 'pickup_month', 'pickup_dow', 'pickup_hour', \n",
    "                  'PUBorough', 'DOBorough', 'CrossSuperborough'\n",
    "                 ]\n",
    "    ddf[categories] = ddf[categories].astype('category')\n",
    "    ddf = ddf.categorize(columns=categories)\n",
    "    ddf = ddf.repartition(partition_size=\"128MB\")\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a8954-1f15-41a5-81f2-2d042fa860f9",
   "metadata": {},
   "source": [
    "## Test Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abab0dd-e0ed-4763-b740-5bc4cf0ecb0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(\n",
    "    worker_vm_types=[\"m6i.4xlarge\"],\n",
    "    scheduler_vm_types=[\"m6i.2xlarge\"],\n",
    "    package_sync=True, # copy local packages,\n",
    "    name=\"dask-engineering-f799f650-0\",\n",
    "    shutdown_on_close=True,  # reuse cluster across runs\n",
    "    show_widget=False,\n",
    "    n_workers=20,\n",
    "    use_best_zone=True,\n",
    "    account=\"dask-engineering\",\n",
    "    )\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a211755-d0f2-410f-9089-892cf09ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616048f-3b09-458a-9e97-8fa04b3500f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = load_data()\n",
    "ddf = make_taxi_data(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554e6e0-0c08-4f4e-ab2e-408abfeb2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e57258-b913-4bf1-bf42-6c919f18de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02f30d-4414-4447-9e84-fa76df9c2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55aa35-3bc7-45d1-b821-edd64e0d73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_parquet(\"s3://prefect-dask-examples/nyc-uber-lyft/feature_table_fixed_upper_bound.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ac5f8-a94b-484b-bd7c-b256d1272d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa882b6-fd67-413f-af4c-99b3a438d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cfdb8dd-bbda-4757-813c-88e652d4d937",
   "metadata": {},
   "source": [
    "CancelledError                            Traceback (most recent call last)\n",
    "Cell In[10], line 2\n",
    "      1 ddf = load_data()\n",
    "----> 2 ddf = make_taxi_data(ddf)\n",
    "\n",
    "Cell In[7], line 33, in make_taxi_data(ddf)\n",
    "     28 categories = ['hvfhs_license_num', 'PULocationID', \"DOLocationID\", 'wav_request_flag',\n",
    "     29               'accessible_vehicle', 'pickup_month', 'pickup_dow', 'pickup_hour', \n",
    "     30               'PUBorough', 'DOBorough', 'CrossSuperborough'\n",
    "     31              ]\n",
    "     32 ddf2[categories] = ddf2[categories].astype('category')\n",
    "---> 33 ddf2 = ddf2.categorize(columns=categories)\n",
    "     34 ddf2 = ddf2.repartition(partition_size=\"128MB\")\n",
    "     35 return ddf2\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/dask/dataframe/core.py:5050, in DataFrame.categorize(self, columns, index, split_every, **kwargs)\n",
    "   5048 @wraps(categorize)\n",
    "   5049 def categorize(self, columns=None, index=None, split_every=None, **kwargs):\n",
    "-> 5050     return categorize(\n",
    "   5051         self, columns=columns, index=index, split_every=split_every, **kwargs\n",
    "   5052     )\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/dask/dataframe/categorical.py:152, in categorize(df, columns, index, split_every, **kwargs)\n",
    "    149 dsk.update(df.dask)\n",
    "    151 # Compute the categories\n",
    "--> 152 categories, index = compute_as_if_collection(\n",
    "    153     df.__class__, dsk, (prefix, 0), **kwargs\n",
    "    154 )\n",
    "    156 # some operations like get_dummies() rely on the order of categories\n",
    "    157 categories = {k: v.sort_values() for k, v in categories.items()}\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/dask/base.py:342, in compute_as_if_collection(cls, dsk, keys, scheduler, get, **kwargs)\n",
    "    340 schedule = get_scheduler(scheduler=scheduler, cls=cls, get=get)\n",
    "    341 dsk2 = optimization_function(cls)(dsk, keys, **kwargs)\n",
    "--> 342 return schedule(dsk2, keys, **kwargs)\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/distributed/client.py:3125, in Client.get(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\n",
    "   3123         should_rejoin = False\n",
    "   3124 try:\n",
    "-> 3125     results = self.gather(packed, asynchronous=asynchronous, direct=direct)\n",
    "   3126 finally:\n",
    "   3127     for f in futures.values():\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/distributed/client.py:2294, in Client.gather(self, futures, errors, direct, asynchronous)\n",
    "   2292 else:\n",
    "   2293     local_worker = None\n",
    "-> 2294 return self.sync(\n",
    "   2295     self._gather,\n",
    "   2296     futures,\n",
    "   2297     errors=errors,\n",
    "   2298     direct=direct,\n",
    "   2299     local_worker=local_worker,\n",
    "   2300     asynchronous=asynchronous,\n",
    "   2301 )\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/distributed/utils.py:339, in SyncMethodMixin.sync(self, func, asynchronous, callback_timeout, *args, **kwargs)\n",
    "    337     return future\n",
    "    338 else:\n",
    "--> 339     return sync(\n",
    "    340         self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n",
    "    341     )\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/distributed/utils.py:406, in sync(loop, func, callback_timeout, *args, **kwargs)\n",
    "    404 if error:\n",
    "    405     typ, exc, tb = error\n",
    "--> 406     raise exc.with_traceback(tb)\n",
    "    407 else:\n",
    "    408     return result\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/distributed/utils.py:379, in sync.<locals>.f()\n",
    "    377         future = asyncio.wait_for(future, callback_timeout)\n",
    "    378     future = asyncio.ensure_future(future)\n",
    "--> 379     result = yield future\n",
    "    380 except Exception:\n",
    "    381     error = sys.exc_info()\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/tornado/gen.py:769, in Runner.run(self)\n",
    "    766 exc_info = None\n",
    "    768 try:\n",
    "--> 769     value = future.result()\n",
    "    770 except Exception:\n",
    "    771     exc_info = sys.exc_info()\n",
    "\n",
    "File ~/Documents/projects/dask-xgboost-nyctaxi/.venv/lib/python3.9/site-packages/distributed/client.py:2158, in Client._gather(self, futures, errors, direct, local_worker)\n",
    "   2156     else:\n",
    "   2157         raise exception.with_traceback(traceback)\n",
    "-> 2158     raise exc\n",
    "   2159 if errors == \"skip\":\n",
    "   2160     bad_keys.add(key)\n",
    "\n",
    "CancelledError: ('get-categories-agg-6b48ff9de1b47c45db7e9d3da7ac964d', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
