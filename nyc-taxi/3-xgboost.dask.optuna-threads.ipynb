{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0652f264-b2c3-43b2-9310-7c00e50e7cdc",
   "metadata": {},
   "source": [
    "# XGBoost.Dask in many threads\n",
    "\n",
    "Sometimes we want to train many large XGBoost models in parallel.  We do so in this example with ...\n",
    "\n",
    "1.  The `xgboost.dask` project to do large training runs\n",
    "2.  Optuna to do hyper-parameter-optimization\n",
    "3.  A thread pool, to run many of these in parallel\n",
    "4.  Coiled to launch Dask clusters (but you could swap in your favorite Dask deployment technology as you like)\n",
    "\n",
    "Using `xgboost.dask` from many threads tooks a couple of small tweaks across projects.  This notebook resulted in the following PRs and issues:\n",
    "\n",
    "-  https://github.com/dask/distributed/issues/7377\n",
    "-  https://github.com/dask/dask/pull/9723\n",
    "-  https://github.com/dask/distributed/pull/7369\n",
    "-  https://github.com/dmlc/xgboost/pull/8558 (mostly cosmetic, not necessary)\n",
    "-  Also something in Coiled to allow package_sync to be thread-safe, should be released by 2022-12-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b929987-5089-4c13-be35-c1812d65fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from distributed import Client\n",
    "import dask.dataframe as dd\n",
    "from coiled import Cluster\n",
    "import coiled\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "\n",
    "from dask_ml.datasets import make_classification_df\n",
    "from dask_ml.model_selection import train_test_split, KFold\n",
    "from dask_ml.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780815b1-fa82-43c1-93c0-572f0f6182c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coiled: 0.2.55\n",
      "dask: 2022.12.0+13.g0d8e12be\n",
      "dask.distributed: 2022.12.0+17.gf8302593\n",
      "optuna: 3.0.4\n",
      "xgboost: 1.7.2\n",
      "coiled: 0.2.55\n"
     ]
    }
   ],
   "source": [
    "import dask, coiled\n",
    "print(\"coiled:\", coiled.__version__)\n",
    "print(\"dask:\", dask.__version__)\n",
    "print(\"dask.distributed:\", dask.distributed.__version__)\n",
    "print(\"optuna:\", optuna.__version__)\n",
    "print(\"xgboost:\", xgb.__version__)\n",
    "print(\"coiled:\", coiled.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7290204f-e5db-4c60-be9c-ea5173dcc300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PULocationID            float64\n",
       "DOLocationID            float64\n",
       "trip_miles              float64\n",
       "trip_time               float64\n",
       "base_passenger_fare     float64\n",
       "tolls                   float64\n",
       "bcf                     float64\n",
       "sales_tax               float64\n",
       "congestion_surcharge    float64\n",
       "airport_fee             float64\n",
       "tips                    float64\n",
       "driver_pay              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.read_parquet(\"s3://coiled-datasets/uber-lyft-tlc/*.parquet\", use_nullable_dtypes=False).select_dtypes(exclude=['string', 'category', 'datetime64[ns]']).astype(\"float\").dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd461c3-e551-420a-a3e1-1ca2515bb613",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8574767-4b4c-4c82-a31d-448bd2d6ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "def load_data():\n",
    "    s3_uri = \"s3://coiled-datasets/uber-lyft-tlc/*.parquet\"\n",
    "    nyc_taxi = (\n",
    "        dd.read_parquet(s3_uri,) #use_nullable_dtypes=True)\n",
    "            .select_dtypes(exclude=[\"string\", \"category\"])\n",
    "    )\n",
    "\n",
    "    nyc_taxi[\"pickup_hour\"] = nyc_taxi[\"pickup_datetime\"].dt.hour\n",
    "\n",
    "    cols = nyc_taxi.select_dtypes(include=\"datetime64[ns]\").columns.tolist()\n",
    "    nyc_taxi[cols] = nyc_taxi[cols].astype(int).div(1e9).astype(int)\n",
    "    nyc_taxi[\"trip_time\"] = nyc_taxi[\"dropoff_datetime\"] - nyc_taxi[\"pickup_datetime\"]\n",
    "    nyc_taxi = nyc_taxi.drop(columns=[\n",
    "        # \"dropoff_datetime\", # outcome\n",
    "        \"base_passenger_fare\", # outcome\n",
    "        \"driver_pay\",  # outcome\n",
    "        \"sales_tax\", # outcome\n",
    "        \"airport_fee\", # bad data (need to fix)\n",
    "    ])\n",
    "    nyc_taxi = nyc_taxi.astype(\"float\")\n",
    "    # nyc_taxi = nyc_taxi.reset_index(drop=True)\n",
    "    # nyc_taxi = nyc_taxi.categorize(\n",
    "    #     columns=[ ]\n",
    "    # )\n",
    "    # encoded_vars = []\n",
    "    # for col in [[\"shared_request_flag\"],[\"shared_match_flag\"],[\"wav_request_flag\"],[\"wav_match_flag\"]]:\n",
    "    #     temp = OneHotEncoder().fit_transform(nyc_taxi[col])\n",
    "    #     temp = temp.reset_index().set_index(\"request_datetime\")\n",
    "    #     encoded_vars.append(temp)\n",
    "        \n",
    "\n",
    "    # nyc_taxi = nyc_taxi.drop(columns=cols2)\n",
    "    # nyc_taxi = nyc_taxi.reset_index().set_index(\"request_datetime\")\n",
    "    # encoded = dd.concat(encoded_vars, axis=1)\n",
    "    # nyc_taxi = dd.concat([nyc_taxi, encoded], axis=1)\n",
    "\n",
    "    \n",
    "    X = nyc_taxi.drop(columns=[\"trip_time\"])\n",
    "    y = nyc_taxi[\"trip_time\"]\n",
    "    return X.to_dask_array(lengths=True), y.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf70461-6ab3-429b-a0a1-29f6b162894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_options = dict(\n",
    "    n_splits = 5 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6743a468-e8fa-409f-80a6-72cee3fee61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_estimate(trial_number, clf_params, n_splits=5):\n",
    "    thread_id = threading.get_ident()\n",
    "    with coiled.Cluster(\n",
    "        package_sync=True, # copy local packages\n",
    "        # name=\"xgb-nyc-taxi-\" + str(thread_id), \n",
    "        name=\"xgb-nyc-taxi-11005882368\",\n",
    "        shutdown_on_close=False,  # reuse cluster across runs\n",
    "        show_widget=False,\n",
    "        n_workers=64,\n",
    "        worker_memory=\"16 GiB\",\n",
    "        account=\"dask-engineering\",\n",
    "        backend_options={\"region\": \"us-east-2\", \"spot\": True, \"spot_on_demand_fallback\": True}\n",
    "    ) as cluster:\n",
    "#     with LocalCluster() as cluster:  # for testing\n",
    "        with Client(cluster) as client:\n",
    "            # with client.as_current():  # this should maybe go away.  See https://github.com/dask/distributed/issues/7377\n",
    "            print(f\"Trial {trial_number} thread {thread_id} Cluster dashboard {cluster.dashboard_link}\")\n",
    "\n",
    "            # Load data here\n",
    "            X, y = load_data()\n",
    "            X = X.persist()\n",
    "            y = y.persist()\n",
    "            cv = KFold(n_splits=5)\n",
    "\n",
    "            val_scores = 0\n",
    "            # for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "                # print(thread_id, f\"Trial {trial_number} KFold {i} started\")\n",
    "            start = datetime.datetime.now()\n",
    "            print(start)\n",
    "\n",
    "            dtrain = DaskDMatrix(client, X, y, enable_categorical=True)\n",
    "            print(\"created d train\")\n",
    "            # dtest = DaskDMatrix(client, X[test], y[test])#, enable_categorical=True)\n",
    "\n",
    "            model = xgb.dask.train(\n",
    "                client,\n",
    "                {\n",
    "                    'verbosity': 1,\n",
    "                    'tree_method': 'hist', \n",
    "                    \"objective\": \"reg:squarederror\",\n",
    "                    **clf_params\n",
    "                },\n",
    "                dtrain,\n",
    "                num_boost_round=4, \n",
    "                evals=[(dtrain, 'train')],\n",
    "                early_stopping_rounds=1\n",
    "            )\n",
    "            print(\"made model\")\n",
    "\n",
    "            predictions = xgb.dask.predict(client, model, dtest)\n",
    "            # predictions = xgb.dask.predict(client, model[\"booster\"], X) #X_test)\n",
    "            print(\"made predictions\")\n",
    "\n",
    "            # actual, predictions = dask.compute(y[test], predictions)\n",
    "            actual, predictions = dask.compute(y, predictions)\n",
    "            assert actual.shape == predictions.shape, (actual.shape, predictions.shape)  # sometimes this is off.  Not sure why.\n",
    "\n",
    "            score = roc_auc_score(actual, predictions)\n",
    "            val_scores += score\n",
    "            end = datetime.datetime.now()\n",
    "            print(end)\n",
    "            print(f\"Trial {trial_number} thread {thread_id} KFold {i}, score: {score}, seconds {((end - start).total_seconds())}\")\n",
    "            print(f\"Trial {trial_number} thread {thread_id} finished\")\n",
    "\n",
    "    return val_scores / n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba600261-672a-447d-a515-2c5cb068637f",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv_estimate(1, {}, data_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0250d23a-4d6c-4758-9200-f47a0a614bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 5, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.99),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 0.9),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.9),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 9),\n",
    "    }\n",
    "    accuracy = cv_estimate(\n",
    "        trial_number=trial.number,\n",
    "        clf_params=params, \n",
    "        n_splits=train_options[\"n_splits\"]\n",
    "    ) \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8f3b355-7e76-423f-8b4c-8e35eb1eb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-17 10:42:40,036]\u001b[0m A new study created in memory with name: no-name-7e46e2c3-3243-435d-8293-93065e7ae1b0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 thread 10888441856 Cluster dashboard http://13.59.164.209:8787\n",
      "2022-12-17 10:43:12.536066\n",
      "created d train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a single study\n",
    "study = optuna.create_study()\n",
    "\n",
    "executor = ThreadPoolExecutor(1)\n",
    "\n",
    "futures = [\n",
    "    executor.submit(study.optimize, objective, n_trials=1) for _ in range(1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00809f7-9f3d-4a14-ba74-3490bad91897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 thread 10972573696 Cluster dashboard http://13.59.164.209:8787\n",
      "2022-12-17 10:28:22.246330\n",
      "created d train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfutures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/xgboost_test/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/mambaforge/envs/xgboost_test/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "futures[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94db657-e8be-47a0-ab3c-442f897cc929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb5f29-3bbf-416f-916c-dcf64caab1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "132568f4-ac1b-43ff-85fd-e5a5324a38a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dask/distributed.git@f83025935383d13033fe0dcd4af2ee689078cb40\n",
      "Collecting git+https://github.com/dask/dask.git@0d8e12be4c2261b3457978c16aba7e893b1cf4a1\n",
      "  Cloning https://github.com/dask/distributed.git (to revision f83025935383d13033fe0dcd4af2ee689078cb40) to /private/var/folders/b5/f_y899x168j7cs2m7szjld5c0000gn/T/pip-req-build-59xg9djo\n",
      "  Cloning https://github.com/dask/dask.git (to revision 0d8e12be4c2261b3457978c16aba7e893b1cf4a1) to /private/var/folders/b5/f_y899x168j7cs2m7szjld5c0000gn/T/pip-req-build-4fs_xccf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/dask/dask.git /private/var/folders/b5/f_y899x168j7cs2m7szjld5c0000gn/T/pip-req-build-4fs_xccf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dask/distributed.git /private/var/folders/b5/f_y899x168j7cs2m7szjld5c0000gn/T/pip-req-build-59xg9djo\n",
      "  Running command git rev-parse -q --verify 'sha^f83025935383d13033fe0dcd4af2ee689078cb40'\n",
      "  Running command git fetch -q https://github.com/dask/distributed.git f83025935383d13033fe0dcd4af2ee689078cb40\n",
      "  Running command git checkout -q f83025935383d13033fe0dcd4af2ee689078cb40\n",
      "  Running command git rev-parse -q --verify 'sha^0d8e12be4c2261b3457978c16aba7e893b1cf4a1'\n",
      "  Running command git fetch -q https://github.com/dask/dask.git 0d8e12be4c2261b3457978c16aba7e893b1cf4a1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/dask/distributed.git to commit f83025935383d13033fe0dcd4af2ee689078cb40\n",
      "  Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git checkout -q 0d8e12be4c2261b3457978c16aba7e893b1cf4a1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Resolved https://github.com/dask/dask.git to commit 0d8e12be4c2261b3457978c16aba7e893b1cf4a1\n",
      "  Installing build dependencies: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: distributed\n",
      "  Building wheel for distributed (pyproject.toml): started\n",
      "  Building wheel for distributed (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for distributed: filename=distributed-2022.12.0+17.gf8302593-py3-none-any.whl size=930327 sha256=836f2e928318995f782223a5f3ad1b374a620fe8c321a65dea1189ed2ea3fc80\n",
      "  Stored in directory: /private/var/folders/b5/f_y899x168j7cs2m7szjld5c0000gn/T/pip-ephem-wheel-cache-p_4_6lg4/wheels/fd/af/11/58dd2291a58d74b51b5fd96dedfb871cc8bba17bef3e91f0b7\n",
      "Successfully built distributed\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: dask\n",
      "  Building wheel for dask (pyproject.toml): started\n",
      "  Building wheel for dask (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for dask: filename=dask-2022.12.0+13.g0d8e12be-py3-none-any.whl size=1142848 sha256=570d6f8a8f844d4d3177ef8491e9f65bc4b4a38c4ad70f7941132ac89cb126d1\n",
      "  Stored in directory: /private/var/folders/b5/f_y899x168j7cs2m7szjld5c0000gn/T/pip-ephem-wheel-cache-jk06mrj7/wheels/c7/85/e5/a40829d209b6d0e02f4b3b79141d47a7eb8737edc6faed4d1c\n",
      "Successfully built dask\n",
      "dask.array<values, shape=(707786079, 11), dtype=float64, chunksize=(957978, 11), chunktype=numpy.ndarray>\n",
      "2022-12-17 14:51:23.171225\n",
      "created d train\n",
      "made model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-17 14:56:40,500 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "with coiled.Cluster(\n",
    "    package_sync=True, # copy local packages\n",
    "    # name=\"xgb-nyc-taxi-\" + str(thread_id), \n",
    "    name=\"xgb-nyc-taxi-11005882368\",\n",
    "    shutdown_on_close=False,  # reuse cluster across runs\n",
    "    show_widget=False,\n",
    "    n_workers=64,\n",
    "    worker_memory=\"16 GiB\",\n",
    "    account=\"dask-engineering\",\n",
    "    backend_options={\"region\": \"us-east-2\", \"spot\": True, \"spot_on_demand_fallback\": True}\n",
    ") as cluster:\n",
    "    client = Client(cluster)\n",
    "    client.restart()\n",
    "\n",
    "    X, y= load_data()\n",
    "    X = X.persist()\n",
    "    y = y.persist()\n",
    "    print(X)\n",
    "\n",
    "    cv = KFold(n_splits=5)\n",
    "\n",
    "    val_scores = 0\n",
    "    # for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        # print(thread_id, f\"Trial {trial_number} KFold {i} started\")\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "\n",
    "    dtrain = DaskDMatrix(client, X, y)#, enable_categorical=True)\n",
    "    print(\"created d train\")\n",
    "    # dtest = DaskDMatrix(client, X[test], y[test])#, enable_categorical=True)\n",
    "    params = {\n",
    "        # 'n_estimators': 5,\n",
    "        'learning_rate': 0.99,\n",
    "        'subsample':  0.9,\n",
    "        'max_depth': 10,\n",
    "        'colsample_bytree': 0.9,\n",
    "        'min_child_weight': 9,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    model = xgb.dask.train(\n",
    "        client,\n",
    "        {\n",
    "            'verbosity': 1,\n",
    "            # 'tree_method': 'hist', \n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            **params\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=4, \n",
    "        evals=[(dtrain, 'train')],\n",
    "        early_stopping_rounds=1\n",
    "    )\n",
    "    print(\"made model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5f226-2762-4ac3-bdae-464e8b5a2c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ea963-4838-4c53-8507-fc6ab93c77ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
