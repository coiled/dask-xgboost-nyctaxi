{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0652f264-b2c3-43b2-9310-7c00e50e7cdc",
   "metadata": {},
   "source": [
    "# Parallelize HPO of XGBoost with Optuna and Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179b003-e075-4be5-bde1-4ce90b7d726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coiled account\n",
    "ACCOUNT = \"dask-engineering\"\n",
    "# Location of feature table\n",
    "FILEPATH = \"s3://coiled-datasets/prefect-dask/nyc-uber-lyft/feature_table.parquet\"\n",
    "# Dask worker instance size\n",
    "WORKER_INSTANCE_TYPE = \"m6i.xlarge\"\n",
    "# Dict for collecting clusters for shutting down when done\n",
    "CLUSTERS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b929987-5089-4c13-be35-c1812d65fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import threading\n",
    "import joblib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from distributed import Client, wait\n",
    "import dask.dataframe as dd\n",
    "import coiled\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd461c3-e551-420a-a3e1-1ca2515bb613",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf70461-6ab3-429b-a0a1-29f6b162894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we subset data for cross-validation\n",
    "def make_cv_splits(df, num_folds):\n",
    "    frac = [1 / num_folds] * num_folds\n",
    "    splits = df.random_split(frac, shuffle=True)\n",
    "    for i in range(num_folds):\n",
    "        train = [splits[j] for j in range(num_folds) if j != i]\n",
    "        test = splits[i]\n",
    "        yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132568f4-ac1b-43ff-85fd-e5a5324a38a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(trial_number, study_params, n_splits=5):\n",
    "    thread_id = threading.get_ident()\n",
    "    cluster_name = \"xgb-nyc-taxi-gbh-\" + str(thread_id)\n",
    "\n",
    "    cluster = CLUSTERS.get(cluster_name, None)\n",
    "    cluster = coiled.Cluster(\n",
    "        worker_vm_types=[WORKER_INSTANCE_TYPE],\n",
    "        scheduler_vm_types=[\"m6i.2xlarge\"],\n",
    "        package_sync=True,  # copy local packages,\n",
    "        name=cluster_name,\n",
    "        shutdown_on_close=False,  # reuse cluster across runs\n",
    "        show_widget=False,\n",
    "        n_workers=20,\n",
    "        use_best_zone=True,\n",
    "        account=ACCOUNT,\n",
    "        backend_options={\"region\": \"us-east-2\", \"spot\": True},\n",
    "        scheduler_options={\"idle_timeout\": \"5 minutes\"},\n",
    "    )\n",
    "\n",
    "    CLUSTERS[cluster_name] = cluster\n",
    "\n",
    "    with Client(cluster) as client:\n",
    "        with client.as_current():\n",
    "\n",
    "            # Load and pre-process the DataFrame\n",
    "            ddf = dd.read_parquet(FILEPATH)\n",
    "            categorical_vars = ddf.select_dtypes(include=\"category\").columns.tolist()\n",
    "            ddf = ddf.categorize(columns=categorical_vars)\n",
    "            float_cols = ddf.select_dtypes(include=\"float\").columns.tolist()\n",
    "            ddf[float_cols] = ddf[float_cols].astype(np.float32).persist()\n",
    "\n",
    "            val_scores = []\n",
    "\n",
    "            for train, test in make_cv_splits(ddf, n_splits):\n",
    "                train = dd.concat(train)\n",
    "\n",
    "                y_train = train[\"trip_time\"].to_frame().persist()\n",
    "                X_train = train.drop(columns=[\"trip_time\"]).persist()\n",
    "\n",
    "                # Make the test data\n",
    "                y_test = test[\"trip_time\"].to_frame().persist()\n",
    "                X_test = test.drop(columns=\"trip_time\").persist()\n",
    "                dtrain = DaskDMatrix(client, X_train, y_train, enable_categorical=True)\n",
    "\n",
    "                model = xgb.dask.train(\n",
    "                    client,\n",
    "                    {\"tree_method\": \"hist\", **study_params},\n",
    "                    dtrain,\n",
    "                    num_boost_round=4,\n",
    "                    evals=[(dtrain, \"train\")],\n",
    "                )\n",
    "                predictions = xgb.dask.predict(client, model, X_test)\n",
    "\n",
    "                score = mean_squared_error(\n",
    "                    y_test.to_dask_array(lengths=True).reshape(\n",
    "                        -1,\n",
    "                    ),\n",
    "                    predictions.to_dask_array(lengths=True),\n",
    "                    squared=False,\n",
    "                )\n",
    "                val_scores.append(score)\n",
    "            return np.mean(val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250d23a-4d6c-4758-9200-f47a0a614bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 75, 125),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.5, 0.7),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 6),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 0, 2),\n",
    "        \"max_cat_to_onehot\": trial.suggest_int(\"max_cat_to_onehot\", 1, 10),\n",
    "    }\n",
    "    rmse = train_model(\n",
    "        trial_number=trial.number,\n",
    "        study_params=params,\n",
    "    )\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3b355-7e76-423f-8b4c-8e35eb1eb022",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a single study\n",
    "start = datetime.now()\n",
    "study = optuna.create_study(study_name=\"parallel-nyc-travel-time-model\")\n",
    "\n",
    "with ThreadPoolExecutor(10) as executor:\n",
    "    futures = [\n",
    "        executor.submit(study.optimize, objective, n_trials=5) for _ in range(10)\n",
    "    ]\n",
    "print(f\"Total time:  {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64485e2-eeb7-4eb2-8661-f8aa4a2e519e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tear down running clusters\n",
    "\n",
    "for c in CLUSTERS.values():\n",
    "    c.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f913de4-9988-49d9-9b23-f5a90ecb9973",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5a2bf-c88c-4266-9738-73cb6fcb9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6da34-582c-4e2d-9b58-89a5369d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c41f7-afa1-44a7-ab36-d1ed15a4bd48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to save the results of your study to examine later.\n",
    "\n",
    "joblib.dump(study, \"data/study_many_threads.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e308a-9569-40b1-9557-c6e16234be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "fig.legend(loc=\"upper right\")\n",
    "plt.savefig(\"data/optimization_history_study_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27b350-1f49-40aa-bf63-ed8df919d602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
