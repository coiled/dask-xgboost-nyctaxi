{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0652f264-b2c3-43b2-9310-7c00e50e7cdc",
   "metadata": {},
   "source": [
    "# HPO of XGBoost with Optuna and Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b929987-5089-4c13-be35-c1812d65fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "from distributed import Client, wait\n",
    "import dask.dataframe as dd\n",
    "import coiled\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from xgboost.core import XGBoostError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7179b003-e075-4be5-bde1-4ce90b7d726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT=\"dask-engineering\"                                  # <-- This is your account\n",
    "FILEPATH=\"s3://coiled-datasets/prefect-dask/nyc-uber-lyft/feature_table.parquet\"  # <-- Location of the feature table\n",
    "WORKER_INSTANCE_TYPE = \"m6i.4xlarge\"                        # <-- EC2 instance size\n",
    "CLUSTER_NAME = f\"nyc-uber-lyft-{uuid.uuid1().hex}\"          # <-- This allows us to resuse the cluster across trials\n",
    "CLUSTERS = {}    # Dict for collecting clusters for shutting down when done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd461c3-e551-420a-a3e1-1ca2515bb613",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf70461-6ab3-429b-a0a1-29f6b162894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we subset data for cross-validation\n",
    "def _make_cv_split(df, num_folds):\n",
    "    frac = [1 / num_folds]*num_folds\n",
    "    splits = df.random_split(frac, shuffle=True)\n",
    "    for i in range(num_folds):\n",
    "        train = [splits[j] for j in range(num_folds) if j != i]\n",
    "        test = splits[i]\n",
    "        yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132568f4-ac1b-43ff-85fd-e5a5324a38a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(trial_number, study_params, n_splits=5):\n",
    "\n",
    "    cluster = CLUSTERS.get(CLUSTER_NAME, None)\n",
    "    if cluster is None:\n",
    "        cluster = coiled.Cluster(\n",
    "            worker_vm_types=[WORKER_INSTANCE_TYPE],\n",
    "            scheduler_vm_types=[\"m6i.2xlarge\"],\n",
    "            package_sync=True, # copy local packages\n",
    "            name=CLUSTER_NAME,\n",
    "            shutdown_on_close=True,\n",
    "            show_widget=False,\n",
    "            n_workers=10,\n",
    "            use_best_zone=True,\n",
    "            account=ACCOUNT,\n",
    "            backend_options={\"region\": \"us-east-2\", \"spot\": True},\n",
    "            scheduler_options={\"idle_timeout\": \"10 minutes\"},\n",
    "            )\n",
    "    CLUSTERS[CLUSTER_NAME] = cluster\n",
    "\n",
    "    print(\"starting run\")\n",
    "    with Client(cluster) as client:\n",
    "        \n",
    "        # Load and pre-process the DataFrame\n",
    "        ddf = dd.read_parquet(FILEPATH)\n",
    "        categorical_vars = ddf.select_dtypes(include=\"category\").columns.tolist()\n",
    "        ddf = ddf.categorize(columns=categorical_vars)     # We need to categories to be `known`\n",
    "        float_cols = ddf.select_dtypes(include=\"float\").columns.tolist()\n",
    "        ddf[float_cols] = ddf[float_cols].astype(np.float32).persist()  # Under the hood, XGBoost converts floats to `float32`\n",
    "        \n",
    "        val_scores = []\n",
    "\n",
    "        for i, (train, test) in enumerate(_make_cv_split(ddf, n_splits)):\n",
    "            print(f\"Starting training run {i}\")\n",
    "            start = datetime.datetime.now()\n",
    "            train = dd.concat(train)\n",
    "\n",
    "            try:\n",
    "                assert all(train[c].cat.known for c in categorical_vars)\n",
    "                assert all(test[c].cat.known for c in categorical_vars)\n",
    "            except Exception as e:\n",
    "                cluster.shutdown()\n",
    "                raise RuntimeError(f\"Categorical_vars are not known\")\n",
    "\n",
    "            y_train = train['trip_time'].to_frame().persist()\n",
    "            X_train = train.drop(columns=['trip_time']).persist()\n",
    "            \n",
    "            # Make the test data\n",
    "            y_test = test['trip_time'].to_frame().persist()\n",
    "            X_test = test.drop(columns='trip_time').persist()\n",
    "\n",
    "            try:\n",
    "                print(\"Make dtrain\")\n",
    "                dtrain = DaskDMatrix(client, X_train, y_train, enable_categorical=True)\n",
    "\n",
    "                print(\"Training model\")\n",
    "                model = xgb.dask.train(\n",
    "                    client,\n",
    "                    {\n",
    "                        'verbosity': 1,\n",
    "                        'tree_method': 'hist', \n",
    "                        \"objective\": \"reg:squarederror\",\n",
    "                        **study_params\n",
    "                    },\n",
    "                    dtrain,\n",
    "                    num_boost_round=4,\n",
    "                    evals=[(dtrain, \"train\")],\n",
    "                )\n",
    "\n",
    "                print(\"Make predictions\")\n",
    "                predictions = xgb.dask.predict(client, model, X_test)\n",
    "\n",
    "                # Materialize the predictions and y_test\n",
    "                y_test = y_test.compute().to_numpy().reshape(-1,)\n",
    "                predictions = predictions.compute().to_numpy()\n",
    "\n",
    "                print(\"Score the model\")\n",
    "                score = mean_squared_error(y_test, predictions, squared=False)\n",
    "                \n",
    "                print(f\"rmse_score:  {score}\")\n",
    "                val_scores.append(score)\n",
    "                print(f\"val_scores:  {val_scores}\")\n",
    "                print(f\"Finished training run in:  {datetime.datetime.now() - start} seconds\")\n",
    "\n",
    "            except XGBoostError as e:\n",
    "                print(f\"Trial {i} failed with {e}\")\n",
    "\n",
    "        return np.mean(val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0250d23a-4d6c-4758-9200-f47a0a614bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 125),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.1, 0.9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0, 1),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 3),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0, 1),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 0.5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "    }\n",
    "    rmse = train_model(\n",
    "        trial_number=trial.number,\n",
    "        study_params=params, \n",
    "        n_splits=5,\n",
    "    )\n",
    "    print(f\"final mse:  {rmse}\")\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3b355-7e76-423f-8b4c-8e35eb1eb022",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-12 20:49:46,864]\u001b[0m A new study created in memory with name: nyc-travel-time-model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting run\n",
      "Starting training run 0\n",
      "Make dtrain\n",
      "Training model\n",
      "Make predictions\n",
      "Score the model\n",
      "rmse_score:  543.8064939574474\n",
      "val_scores:  [543.8064939574474]\n",
      "Finished training run in:  0:04:26.521681 seconds\n",
      "Starting training run 1\n",
      "Make dtrain\n",
      "Training model\n",
      "Make predictions\n",
      "Score the model\n",
      "rmse_score:  566.2642121296011\n",
      "val_scores:  [543.8064939574474, 566.2642121296011]\n",
      "Finished training run in:  0:04:16.372493 seconds\n",
      "Starting training run 2\n",
      "Make dtrain\n",
      "Training model\n",
      "Make predictions\n",
      "Score the model\n",
      "rmse_score:  576.8750530144338\n",
      "val_scores:  [543.8064939574474, 566.2642121296011, 576.8750530144338]\n",
      "Finished training run in:  0:04:35.971731 seconds\n",
      "Starting training run 3\n",
      "Make dtrain\n",
      "Training model\n",
      "Make predictions\n"
     ]
    }
   ],
   "source": [
    "# Create a single study and run some trials\n",
    "\n",
    "study = optuna.create_study(study_name=\"nyc-travel-time-model\")    \n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "for c in CLUSTERS.values():\n",
    "    c.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f913de4-9988-49d9-9b23-f5a90ecb9973",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(study.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5a2bf-c88c-4266-9738-73cb6fcb9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6da34-582c-4e2d-9b58-89a5369d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4928342-4749-4e29-bb65-ccabd8190a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c41f7-afa1-44a7-ab36-d1ed15a4bd48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this if you want to save the results of your study to examine later.\n",
    "\n",
    "#joblib.dump(study, \"data/study_m6i4xlarge.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e308a-9569-40b1-9557-c6e16234be43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
