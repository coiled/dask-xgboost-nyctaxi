{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0652f264-b2c3-43b2-9310-7c00e50e7cdc",
   "metadata": {},
   "source": [
    "# Feature Engineering In Advance of HPO\n",
    "\n",
    "Often our data is not optimally arranged for training.  In this notebook we perform basic dataframe operations like filtering out outliers, repartitioning, expanding dates, and so on.  Everything here should be familiar to a modestly experienced pandas user.  It should also vary by dataset.\n",
    "\n",
    "We do encourage a few specific general optimizations:\n",
    "\n",
    "-   Categorization\n",
    "-   Efficient datatypes (like pyarrow strings)\n",
    "-   Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b929987-5089-4c13-be35-c1812d65fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask.config\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff1c43",
   "metadata": {},
   "source": [
    "### Start Coiled cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abab0dd-e0ed-4763-b740-5bc4cf0ecb0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask.config.set({\"dataframe.dtype_backend\": \"pyarrow\"})\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    worker_vm_types=[\"m6i.xlarge\"],\n",
    "    scheduler_vm_types=[\"m6i.large\"],\n",
    "    package_sync=True,  # align remote packages to local ones\n",
    "    n_workers=10,\n",
    "    backend_options={\n",
    "        \"region\": \"us-east-2\",\n",
    "        \"multizone\": True,\n",
    "        \"spot\": True,\n",
    "        \"spot_on_demand_fallback\": True,\n",
    "    },\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23735db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary workaround to https://github.com/dask/dask/issues/9840\n",
    "from distributed import WorkerPlugin\n",
    "\n",
    "\n",
    "class SetPandasOptions(WorkerPlugin):\n",
    "    def setup(self, worker):\n",
    "        pd.set_option(\"string_storage\", \"pyarrow\")\n",
    "\n",
    "\n",
    "pd.set_option(\"string_storage\", \"pyarrow\")  # Set on the client\n",
    "_ = client.register_worker_plugin(SetPandasOptions())  # Set on the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd461c3-e551-420a-a3e1-1ca2515bb613",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/prefect-dask/nyc-uber-lyft/processed_data.parquet\",\n",
    "    index=False,\n",
    "    columns=[\n",
    "        \"hvfhs_license_num\",\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"trip_miles\",\n",
    "        \"trip_time\",\n",
    "        \"tolls\",\n",
    "        \"congestion_surcharge\",\n",
    "        \"airport_fee\",\n",
    "        \"wav_request_flag\",\n",
    "        \"on_scene_datetime\",\n",
    "        \"pickup_datetime\",\n",
    "    ],\n",
    ")\n",
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2621562",
   "metadata": {},
   "source": [
    "The size of the partitions in the input dataset varies substantially, between 22 and 836 MiB. At the cost of reading the whole dataset twice, we must avoid processing the biggest chunks without first breaking them down into a more manageable size.\n",
    "Note that this won't stop the 836 MiB partitions from being read into memory all at once; however, instead of having to crunch the whole processing pipeline on them, if we rechunk early we can break them down and forget them immediately after they have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.repartition(partition_size=\"100MB\")\n",
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1e843",
   "metadata": {},
   "source": [
    "### Postprocess columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.assign(\n",
    "    accessible_vehicle=ddf.on_scene_datetime.isnull(),\n",
    "    pickup_month=ddf.pickup_datetime.dt.month,\n",
    "    pickup_dow=ddf.pickup_datetime.dt.dayofweek,\n",
    "    pickup_hour=ddf.pickup_datetime.dt.hour,\n",
    ")\n",
    "ddf = ddf.drop(columns=[\"on_scene_datetime\", \"pickup_datetime\"])\n",
    "ddf[\"airport_fee\"] = ddf[\"airport_fee\"].replace(\"None\", 0).astype(float).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ac727",
   "metadata": {},
   "source": [
    "### Filter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.dropna(how=\"any\")\n",
    "\n",
    "# Remove outliers\n",
    "# Based on our earlier EDA, we will set the lower bound at zero, which is consistent\n",
    "# with our domain knowledge that no trip should have a duration less than zero.\n",
    "# e calculate the upper_bound and filter the IQR.\n",
    "lower_bound = 0\n",
    "Q3 = ddf[\"trip_time\"].quantile(0.75)\n",
    "upper_bound = Q3 + (1.5 * (Q3 - lower_bound))\n",
    "ddf = ddf.loc[(ddf[\"trip_time\"] >= lower_bound) & (ddf[\"trip_time\"] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef82c9",
   "metadata": {},
   "source": [
    "### Join with domain information\n",
    "\n",
    "Downloaded the \"Taxi Zone Lookup Table (CSV) from [here](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd3bf0-f8c2-4240-b12b-8ee6e4c772c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_lookup = pd.read_csv(\n",
    "    \"s3://coiled-datasets/prefect-dask/nyc-uber-lyft/taxi+_zone_lookup.csv\",\n",
    "    usecols=[\"LocationID\", \"Borough\"],\n",
    ")\n",
    "BOROUGH_MAPPING = {\n",
    "    \"Manhattan\": \"Superborough 1\",\n",
    "    \"Bronx\": \"Superborough 1\",\n",
    "    \"EWR\": \"Superborough 1\",\n",
    "    \"Brooklyn\": \"Superborough 2\",\n",
    "    \"Queens\": \"Superborough 2\",\n",
    "    \"Staten Island\": \"Superborough 3\",\n",
    "    \"Unknown\": \"Unknown\",\n",
    "}\n",
    "\n",
    "taxi_zone_lookup[\"Superborough\"] = [\n",
    "    BOROUGH_MAPPING[k] for k in taxi_zone_lookup[\"Borough\"]\n",
    "]\n",
    "taxi_zone_lookup = taxi_zone_lookup.astype(\n",
    "    {\"Borough\": \"string[pyarrow]\", \"Superborough\": \"string[pyarrow]\"}\n",
    ")\n",
    "taxi_zone_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.merge(\n",
    "    ddf,\n",
    "    taxi_zone_lookup,\n",
    "    left_on=\"PULocationID\",\n",
    "    right_on=\"LocationID\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "ddf = ddf.rename(columns={\"Borough\": \"PUBorough\", \"Superborough\": \"PUSuperborough\"})\n",
    "ddf = ddf.drop(columns=\"LocationID\")\n",
    "\n",
    "ddf = dd.merge(\n",
    "    ddf,\n",
    "    taxi_zone_lookup,\n",
    "    left_on=\"DOLocationID\",\n",
    "    right_on=\"LocationID\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "ddf = ddf.rename(columns={\"Borough\": \"DOBorough\", \"Superborough\": \"DOSuperborough\"})\n",
    "ddf = ddf.drop(columns=\"LocationID\")\n",
    "\n",
    "ddf[\"PUSuperborough_DOSuperborough\"] = ddf.PUSuperborough.str.cat(\n",
    "    ddf.DOSuperborough, sep=\"-\"\n",
    ")\n",
    "ddf = ddf.drop(columns=[\"PUSuperborough\", \"DOSuperborough\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21bd3d5",
   "metadata": {},
   "source": [
    "### Categorize\n",
    "Convert column data to categories, with homogeneous domains across partitions.\n",
    "\n",
    "categorize() works in four steps:\n",
    "1. compute the whole input dataframe\n",
    "2. collect local categorical domains from each partition and send them back to the client\n",
    "3. on the client, generate dataframe-wide categorical domains as the boolean union of the local ones\n",
    "4. return a new graph where each chunk is converted to category using the global domains\n",
    "\n",
    "This means that, once you compute the output of categorize(), you will have computed the\n",
    "whole thing twice. To avoid this, we persist() to hide the graph so far from categorize().\n",
    "\n",
    "This however has in turn the drawback that we need to have the whole dataframe in memory\n",
    "at once. In order to reduce the amount of memory we need for it, we *locally* categorize\n",
    "each partition - which in turn drastically reduces its size - before we persist.\n",
    "\n",
    "Read more: https://github.com/dask/dask/issues/9847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b66acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"hvfhs_license_num\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"wav_request_flag\",\n",
    "    \"accessible_vehicle\",\n",
    "    \"pickup_month\",\n",
    "    \"pickup_dow\",\n",
    "    \"pickup_hour\",\n",
    "    \"PUBorough\",\n",
    "    \"DOBorough\",\n",
    "    \"PUSuperborough_DOSuperborough\",\n",
    "]\n",
    "\n",
    "ddf = ddf.astype(dict.fromkeys(categories, \"category\"))\n",
    "ddf = ddf.persist()\n",
    "# This blocks until the whole workload so far has been persisted\n",
    "ddf = ddf.categorize(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33887d19",
   "metadata": {},
   "source": [
    "### Prepare for output\n",
    "Make all partitions the same size (they aren't due to the row filtering earlier on) and define the final partition size.\n",
    "Again, repartition() needs to compute its whole input twice, since the latest persist().\n",
    "In order to avoid this, we persist() immediately before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.persist()\n",
    "ddf = ddf.repartition(partition_size=\"100MB\")\n",
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fde46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8008742",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae76b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary workaround to https://github.com/apache/arrow/issues/33727\n",
    "ddf = ddf.astype(\n",
    "    {\n",
    "        col: pd.CategoricalDtype(dt.categories.astype(object))\n",
    "        for col, dt in ddf.dtypes.items()\n",
    "        if isinstance(dt, pd.CategoricalDtype)\n",
    "        and dt.categories.dtype == \"string[pyarrow]\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d929b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_parquet(\n",
    "    \"s3://coiled-datasets/prefect-dask/nyc-uber-lyft/feature_table.parquet\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68922a18-0f7b-465f-a567-4762d0023ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
